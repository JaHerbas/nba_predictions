{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "from pandas import read_csv\n",
    "from datetime import datetime, timedelta\n",
    "from timeit import default_timer\n",
    "\n",
    "df_team_bx = read_csv('data/team_boxscore_stats_1997-19.csv')\n",
    "df_player_bx = read_csv('data/player_boxscore_stats_1997-19_adv.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label: W/L\n",
    "Features: \n",
    "\n",
    "home win/away win\n",
    "rest days \n",
    "win/losee rate for last 10 games\n",
    "won last game\n",
    "p1_four_factors*4 (*5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Season1997...\n",
      "Processing Season1999...\n",
      "Processing Season1999...\n",
      "Processing Season2000...\n",
      "Processing Season2001...\n",
      "Processing Season2002...\n",
      "Processing Season2003...\n",
      "Processing Season2004...\n",
      "Processing Season2005...\n",
      "Processing Season2006...\n",
      "Processing Season2007...\n",
      "Processing Season2008...\n",
      "Processing Season2009...\n",
      "Processing Season2010...\n",
      "Processing Season2011...\n",
      "Processing Season2012...\n",
      "Processing Season2013...\n",
      "Processing Season2014...\n",
      "Processing Season2015...\n",
      "Processing Season2016...\n",
      "Processing Season2017...\n",
      "Processing Season2018...\n",
      "Cleaning incomplete samples...\n",
      "25694  Samples Generated in  0:08:12.982610\n"
     ]
    }
   ],
   "source": [
    "HOME = 'H'\n",
    "AWAY = 'A'\n",
    "REQ_PLAYER_STATS = [\n",
    "        \"PLAYER_ID\",\n",
    "        \"PLAYER_NAME\",\n",
    "        \"OREB_PCT\",\n",
    "        \"DREB_PCT\",\n",
    "        \"EFG_PCT\",\n",
    "        \"AST_TO\",\n",
    "        \"W_PCT\",\n",
    "        \"MIN\"\n",
    "]\n",
    "\n",
    "\n",
    "def features():\n",
    "    features_type = {\n",
    "        #'WINNER': str,\n",
    "        'H_RD': int,  # Rest days\n",
    "        # 'H_WR': float, # Win ratio for last 10 games\n",
    "        'H_WLG': bool,  # Won last game\n",
    "        'H_EFG': float,  # Effective field goal %\n",
    "        'H_OREB': float,  # Offesnie RB %\n",
    "        'H_DREB': float,  # Defensive RB %\n",
    "        'H_AST_TO': float,  # Assit to Turn over ratio\n",
    "        #'H_W': float,  # win percentage\n",
    "        'A_RD': int,  # Rest days\n",
    "        # 'A_WR': float, # Win ratio for last 10 games\n",
    "        'A_WLG': bool,  # Won last game\n",
    "        'A_EFG': float,  # Effective field goal %\n",
    "        'A_OREB': float,  # Offesnie RB %\n",
    "        'A_DREB': float,  # Defensive RB %\n",
    "        'A_AST_TO': float,  # Assit to Turn over ratio\n",
    "        #'A_W': float  # win percentage\n",
    "    }\n",
    "    return features_type\n",
    "\n",
    "\n",
    "def check_court(match):\n",
    "    return AWAY if match.split()[1] == '@' else HOME\n",
    "\n",
    "def check_winner(game_inf):\n",
    "    winner = ''\n",
    "    court = check_court(game_inf['MATCHUP'])\n",
    "    if court == HOME:\n",
    "        winner = HOME if game_inf['WL'] == 'W' else AWAY\n",
    "    else:\n",
    "        winner = AWAY if game_inf['WL'] == 'W' else HOME\n",
    "    \n",
    "    return winner\n",
    "\n",
    "def guess_starters(players_bx, game_inf):\n",
    "    return players_bx[(\n",
    "        players_bx.GAME_ID == game_inf['GAME_ID']) & (players_bx.TEAM_ID == game_inf['TEAM_ID'])].head(\n",
    "        10).sort_values(\n",
    "        by='MIN', ascending=False) #TODO: clean this\n",
    "\n",
    "\n",
    "def assign_features(sample, game_inf, teams_meta, player_stats):\n",
    "    court_prefix = check_court(game_inf['MATCHUP']) + \"_\"\n",
    "    game_id, team_id = game_inf['GAME_ID'], game_inf['TEAM_ID']\n",
    "    game_date = datetime.strptime(game_inf['GAME_DATE'][:10], '%Y-%m-%d')\n",
    "\n",
    "    weights = player_stats['MIN'] / (player_stats['MIN'].sum())\n",
    "    sample[court_prefix + 'RD'] = (game_date - teams_meta[team_id]['last_game_dt']).days # Rest days\n",
    "    # sample[court_prefix + 'WR'] =  # Win ratio for last 10 games\n",
    "    sample[court_prefix + 'WLG'] = teams_meta[team_id]['WLG']\n",
    "    sample[court_prefix + 'EFG'] = (weights * player_stats['EFG_PCT']).sum()  # Effective field goal %\n",
    "    sample[court_prefix + 'OREB'] = (weights * player_stats['OREB_PCT']).sum()  # Offesnie RB %\n",
    "    sample[court_prefix + 'DREB'] = (weights * player_stats['DREB_PCT']).sum()  # Defensive RB %\n",
    "    sample[court_prefix + 'AST_TO'] = (weights * player_stats['AST_TO']).sum()  # Assit to Turn over ratio\n",
    "    sample[court_prefix + 'W_PCT'] = (weights * player_stats['W_PCT']).sum()\n",
    "\n",
    "start_time = default_timer()\n",
    "x_train = list()\n",
    "y_train = list()\n",
    "season_id = None\n",
    "incomplete_sample = dict() # {'%ID\": %i}\n",
    "\n",
    "for i, game_inf in df_team_bx.iterrows():\n",
    "    if season_id != game_inf['SEASON_ID']: #TODO: SEASON ID, not accuracy, Playoff is another season only for1999 season\n",
    "        season_id = game_inf['SEASON_ID']\n",
    "        teams_meta = dict()  # Keyed by Team ID {'ID': {'last_5_games', 'last_game_dt', 'WLG'}}\n",
    "        yr = game_inf['GAME_DATE'][:4]\n",
    "        file_path = 'data/player_yearly_stats/player_stats_' + str(int(yr) - 1)[-2:] + '-' + yr[-2:] + '.csv'\n",
    "        df_player_stats_hist = read_csv(file_path)\n",
    "        print('Processing Season' + yr + '...')\n",
    "\n",
    "    game_id, team_id  = game_inf['GAME_ID'], game_inf['TEAM_ID']\n",
    "    if not teams_meta.get(team_id):\n",
    "        teams_meta[team_id] = {\n",
    "            'last_game_dt': datetime.strptime(game_inf['GAME_DATE'][:10], '%Y-%m-%d'),\n",
    "            'WLG': True if game_inf['WL'] == 'W' else False\n",
    "        }\n",
    "        continue\n",
    "\n",
    "    #TODO: implement winning streaks23\n",
    "    #     if len(teams_meta.get(team_id, {}).get(last_5_games, [])) < 5:\n",
    "    #         team_meta[team_id]['last_5_games'] = team_meta[team_id].get('last_5_games',[]) + []\n",
    "    #         continue\n",
    "\n",
    "    if game_id in incomplete_sample:\n",
    "        sample = x_train[incomplete_sample[game_id]]\n",
    "        incomplete_sample.pop(game_id)\n",
    "    else:\n",
    "        sample = features()\n",
    "        x_train.append(sample)\n",
    "        y_train.append(check_winner(game_inf))\n",
    "        incomplete_sample[game_id] = len(x_train) - 1\n",
    "    \n",
    "    starters = guess_starters(df_player_bx, game_inf)\n",
    "    starters_stats = df_player_stats_hist.merge(starters, on='PLAYER_ID', how='right', suffixes =('', '_BX'))\n",
    "\n",
    "    #TODO: Assign residual stats, deal with non-existsent stats\n",
    "    #         if not avg_stats:\n",
    "    #             # do not overwrite\n",
    "    #             player_stats = df_player_stats_cur[df_player_stats_cur.PLAYER_ID == player['PLAYER_ID']]\n",
    "\n",
    "    assign_features(sample, game_inf, teams_meta, starters_stats)\n",
    "    teams_meta[team_id] = {\n",
    "        'last_game_dt': datetime.strptime(game_inf['GAME_DATE'][:10], '%Y-%m-%d'),\n",
    "        'WLG': True if game_inf['WL'] == 'W' else False\n",
    "    }\n",
    "\n",
    "print('Cleaning incomplete samples...')\n",
    "# Clean throw away data\n",
    "for i in sorted(list(incomplete_sample.values()), reverse=True):\n",
    "    x_train.pop(i)\n",
    "    y_train.pop(i)\n",
    "\n",
    "end_time = default_timer()\n",
    "print(len(x_train), ' Samples Generated in ', timedelta(seconds=end_time-start_time))\n",
    "\n",
    "pd.DataFrame(x_train).to_csv('feature_data/x_train_top10players.csv')\n",
    "pd.DataFrame(y_train).to_csv('feature_data/y_train_top10players.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(x_train).to_csv('feature_data/x_train_top7players.csv')\n",
    "pd.DataFrame(y_train).to_csv('feature_data/y_train_top7players.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'H_RD': 1,\n",
       " 'H_WLG': False,\n",
       " 'H_EFG': 0.48755163727959705,\n",
       " 'H_OREB': 0.06142317380352645,\n",
       " 'H_DREB': 0.11754911838790934,\n",
       " 'H_AST_TO': 1.6630142737195635,\n",
       " 'A_RD': int,\n",
       " 'A_WLG': bool,\n",
       " 'A_EFG': float,\n",
       " 'A_OREB': float,\n",
       " 'A_DREB': float,\n",
       " 'A_AST_TO': float,\n",
       " 'H_W_PCT': 0.2025247691015953}"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a LogisticRegression using a training set size of 20555. . .\n",
      "Trained model in 0.2085 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mhsue\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made predictions in 0.0149 seconds.\n",
      "0.7389370853685208 0.651568961323279\n",
      "F1 score and accuracy score for training set: 0.7389 , 0.6516.\n",
      "Made predictions in 0.0050 seconds.\n",
      "F1 score and accuracy score for test set: 0.7358 , 0.6472.\n",
      "\n",
      "Training a SVC using a training set size of 20555. . .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mhsue\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model in 16.3043 seconds\n",
      "Made predictions in 9.7055 seconds.\n",
      "0.7515574214517877 0.6430065677450741\n",
      "F1 score and accuracy score for training set: 0.7516 , 0.6430.\n",
      "Made predictions in 2.3816 seconds.\n",
      "F1 score and accuracy score for test set: 0.7467 , 0.6332.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#for measuring training time\n",
    "from time import time \n",
    "# F1 score (also F-score or F-measure) is a measure of a test's accuracy. \n",
    "#It considers both the precision p and the recall r of the test to compute \n",
    "#the score: p is the number of correct positive results divided by the number of \n",
    "#all positive results, and r is the number of correct positive results divided by \n",
    "#the number of positive results that should have been returned. The F1 score can be \n",
    "#interpreted as a weighted average of the precision and recall, where an F1 score \n",
    "#reaches its best value at 1 and worst at 0.\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "#produces a prediction model in the form of an ensemble of weak prediction models, typically decision tree\n",
    "#import xgboost as xgb\n",
    "#the outcome (dependent variable) has only a limited number of possible values. \n",
    "#Logistic Regression is used when response variable is categorical in nature.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#A random forest is a meta estimator that fits a number of decision tree classifiers \n",
    "#on various sub-samples of the dataset and use averaging to improve the predictive \n",
    "#accuracy and control over-fitting.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#a discriminative classifier formally defined by a separating hyperplane.\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Shuffle and split the dataset into training and testing set.\n",
    "X_train, X_test, Y_train, y_test = train_test_split(pd.DataFrame(x_train).values, np.array(y_train), \n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state = 2,\n",
    "                                                    stratify = np.array(y_train))\n",
    "def train_classifier(clf, X_train, y_train):\n",
    "    ''' Fits a classifier to the training data. '''\n",
    "    \n",
    "    # Start the clock, train the classifier, then stop the clock\n",
    "    start = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time()\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"Trained model in {:.4f} seconds\".format(end - start))\n",
    "\n",
    "    \n",
    "def predict_labels(clf, features, target):\n",
    "    ''' Makes predictions using a fit classifier based on F1 score. '''\n",
    "    \n",
    "    # Start the clock, make predictions, then stop the clock\n",
    "    start = time()\n",
    "    y_pred = clf.predict(features)\n",
    "    \n",
    "    end = time()\n",
    "    # Print and return results\n",
    "    print(\"Made predictions in {:.4f} seconds.\".format(end - start))\n",
    "    \n",
    "    return f1_score(target, y_pred, pos_label='H'), sum(target == y_pred) / float(len(y_pred))\n",
    "\n",
    "\n",
    "def train_predict(clf, X_train, y_train, X_test, y_test):\n",
    "    ''' Train and predict using a classifer based on F1 score. '''\n",
    "    \n",
    "    # Indicate the classifier and the training set size\n",
    "    print(\"Training a {} using a training set size of {}. . .\".format(clf.__class__.__name__, len(X_train)))\n",
    "    \n",
    "    # Train the classifier\n",
    "    train_classifier(clf, X_train, y_train)\n",
    "    \n",
    "    # Print the results of prediction for both training and testing\n",
    "    f1, acc = predict_labels(clf, X_train, y_train)\n",
    "    print(f1, acc)\n",
    "    print(\"F1 score and accuracy score for training set: {:.4f} , {:.4f}.\".format(f1 , acc))\n",
    "\n",
    "    f1, acc = predict_labels(clf, X_test, y_test)\n",
    "    print(\"F1 score and accuracy score for test set: {:.4f} , {:.4f}.\".format(f1 , acc))\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# Initialize the three models (XGBoost is initialized later)\n",
    "clf_A = LogisticRegression(random_state = 42)\n",
    "clf_B = SVC(random_state = 912, kernel='rbf')\n",
    "\n",
    "#Boosting refers to this general problem of producing a very accurate prediction rule \n",
    "#by combining rough and moderately inaccurate rules-of-thumb\n",
    "#clf_C = xgb.XGBClassifier(seed = 82)\n",
    "\n",
    "train_predict(clf_A, X_train, Y_train, X_test, y_test)\n",
    "print('')\n",
    "train_predict(clf_B, X_train, Y_train, X_test, y_test)\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a KNeighborsClassifier using a training set size of 20555. . .\n",
      "Trained model in 0.2852 seconds\n",
      "Made predictions in 1.6027 seconds.\n",
      "0.740139211136891 0.6948674288494283\n",
      "F1 score and accuracy score for training set: 0.7401 , 0.6949.\n",
      "Made predictions in 0.3860 seconds.\n",
      "F1 score and accuracy score for test set: 0.6423 , 0.5824.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_C = KNeighborsClassifier(n_neighbors =8)\n",
    "train_predict(clf_C, X_train, Y_train, X_test, y_test)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a LinearDiscriminantAnalysis using a training set size of 20555. . .\n",
      "Trained model in 0.0559 seconds\n",
      "Made predictions in 0.0020 seconds.\n",
      "0.7362593040736258 0.6500608124543906\n",
      "F1 score and accuracy score for training set: 0.7363 , 0.6501.\n",
      "Made predictions in 0.0010 seconds.\n",
      "F1 score and accuracy score for test set: 0.7363 , 0.6497.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "clf_D = LinearDiscriminantAnalysis()\n",
    "train_predict(clf_D, X_train, Y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a RandomForestClassifier using a training set size of 20555. . .\n",
      "Trained model in 0.9783 seconds\n",
      "Made predictions in 0.1223 seconds.\n",
      "0.7544375 0.61770858671856\n",
      "F1 score and accuracy score for training set: 0.7544 , 0.6177.\n",
      "Made predictions in 0.0364 seconds.\n",
      "F1 score and accuracy score for test set: 0.7507 , 0.6104.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_E = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "train_predict(clf_E, X_train, Y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('test', pd.DataFrame(x_train).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
